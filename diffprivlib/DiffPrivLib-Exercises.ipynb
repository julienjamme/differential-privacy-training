{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736b2ba8-a7f8-4130-a75b-93bb43364fa6",
   "metadata": {},
   "source": [
    "# Introduction to DiffPrivLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd6a0c-3905-42c4-914c-fbe4a6561f8a",
   "metadata": {},
   "source": [
    "[DiffPrivLib](https://diffprivlib.readthedocs.io/en/latest/index.html) is a python library dedicated to differential privacy and machine learning. It is based on `scikit-learn` library. \n",
    "\n",
    "Some other [introduction notebooks](https://github.com/IBM/differential-privacy-library/tree/main/notebooks) are available directly in the official library repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb8d7aa-47a7-4915-b52f-e9d540b1192b",
   "metadata": {},
   "source": [
    "## Step 1: Install the Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834afa6-8506-4542-8abc-15ec5efd988c",
   "metadata": {},
   "source": [
    "DiffPrivLib is available on pypi, it can be installed via the pip command. We will use the latest version of the library: version 0.6.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378676fe-5cfd-4b68-abc8-c10515754ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.7 environment at: /opt/python\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m7 packages\u001b[0m \u001b[2min 105ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m     0 B/172.72 KiB          \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 14.91 KiB/172.72 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 30.91 KiB/172.72 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 46.91 KiB/172.72 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 62.91 KiB/172.72 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------------\u001b[0m\u001b[0m 78.91 KiB/172.72 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)m-------------\u001b[0m\u001b[0m 94.91 KiB/172.72 KiB        \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\u001b[2m----------\u001b[0m\u001b[0m 110.91 KiB/172.72 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)---\u001b[2m-------\u001b[0m\u001b[0m 126.91 KiB/172.72 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-----\u001b[2m-----\u001b[0m\u001b[0m 142.91 KiB/172.72 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)--------\u001b[2m--\u001b[0m\u001b[0m 158.91 KiB/172.72 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[2m\u001b[0m\u001b[0m 172.72 KiB/172.72 KiB       \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m                                                   \u001b[1A\n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 31ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdiffprivlib\u001b[0m\u001b[2m==0.6.6\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "uv pip install diffprivlib==0.6.6 --system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856aa74-0269-4c83-8027-86a05a6d8695",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6486c600-1f9e-4993-9a3c-ace88534f173",
   "metadata": {},
   "source": [
    "### Load penguin dataset\n",
    "In this notebook, we will work with the [penguin dataset](\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\") from [seaborn datasets](https://github.com/mwaskom/seaborn-data).\n",
    "We load the dataset via pandas in a dataframe `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db7497b-8805-4786-bc19-431841d16d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e32ecd-3298-49d7-9a89-932cfe701556",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "df = pd.read_csv(path_to_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fa0dd-b533-484a-95f4-8a18982a01f3",
   "metadata": {},
   "source": [
    "We can look at the first rows of the dataframe to get to know the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c60a81ad-15d4-4686-b096-788abd7c316a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>MALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>40.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>36.7</td>\n",
       "      <td>19.3</td>\n",
       "      <td>193.0</td>\n",
       "      <td>3450.0</td>\n",
       "      <td>FEMALE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
       "3  Adelie  Torgersen             NaN            NaN                NaN   \n",
       "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
       "\n",
       "   body_mass_g     sex  \n",
       "0       3750.0    MALE  \n",
       "1       3800.0  FEMALE  \n",
       "2       3250.0  FEMALE  \n",
       "3          NaN     NaN  \n",
       "4       3450.0  FEMALE  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ebc409-8b4c-49eb-89ee-7fa381dd3270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Adelie', 'Chinstrap', 'Gentoo'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"species\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea87119-a3f5-4dd0-ba49-68363d4b00c4",
   "metadata": {},
   "source": [
    "### Handle null values\n",
    "DiffPrivLib does not allow null values so we will have to remove or convert them. For simplicity, we will just drop the rows with null values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a726b2f-6e70-44e1-bddd-bf5375a37b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344 rows before dropping nulls.\n",
      "333 rows after dropping nulls.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df.shape[0]} rows before dropping nulls.\")\n",
    "df = df.dropna()\n",
    "print(f\"{df.shape[0]} rows after dropping nulls.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec72b6-df00-440b-92a7-ccc415efd27c",
   "metadata": {},
   "source": [
    "### Encode columns for Machine Learning\n",
    "\n",
    "In the following analysis, we will use the `sex` column as a feature column. We encode the `MALE` and `FEMALE` strings in numbers that the models will be able to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75671917-d7d7-42ea-a5a0-a6aae4b369e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sex\"] = df[\"sex\"].map({\"MALE\": 0, \"FEMALE\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02c47d59-7624-41b1-9f59-4ff31f3c6d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.1</td>\n",
       "      <td>18.7</td>\n",
       "      <td>181.0</td>\n",
       "      <td>3750.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adelie</td>\n",
       "      <td>Torgersen</td>\n",
       "      <td>39.5</td>\n",
       "      <td>17.4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
       "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
       "\n",
       "   body_mass_g  sex  \n",
       "0       3750.0    0  \n",
       "1       3800.0    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74635a3-6393-4691-9d04-e74d03976002",
   "metadata": {},
   "source": [
    "## Step 3: Logistic Regression with DiffPrivLib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436a6d5-8697-471b-a3fb-47327cc11a79",
   "metadata": {},
   "source": [
    "We want to predict penguin's species, i.e., Adelie, Chinstrap, or Gentoo, based on bill length, bill depth, flipper length, body mass and sex. \n",
    "Therefore, we will estimate a multiclass logistic regression:\n",
    "\n",
    "$$\n",
    "Pr(Y_i = \\{species_k\\}) = \\frac{e^{\\beta_k X_i}}{1 + \\sum_{j = 1}^3 e^{\\beta_j X_i}}.\n",
    "$$\n",
    "\n",
    "We first split the data between features and target (to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86da38f1-23be-4bc7-ae8d-ec3bbed087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features columns\n",
    "feature_columns = ['bill_length_mm', 'bill_depth_mm', 'sex']\n",
    "\n",
    "# Target column\n",
    "target_columns = ['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41b0a71-38f0-4335-a343-d43eafc84aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature and target dataframe\n",
    "feature_data = df[feature_columns]\n",
    "label_data = df[target_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ecc27a-aa0c-4a4c-85a7-7f43e70a5614",
   "metadata": {},
   "source": [
    "And then split the data to get a training set, i.e. data used to estimate the model's parameter, and a testing set, .i.e. data used to estimate the performance of the model, with the [train_test_split function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#train-test-split) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d54185-0ef5-4b1e-b598-c4da0804af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3e1c687-0aa6-44da-aa14-da7b514b5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define size of the test set\n",
    "TEST_SIZE = 0.2 # use 20% of data for testing\n",
    "RANDOM_STATE = 1 # Seed for random selection of test set\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    feature_data,\n",
    "    label_data,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "y_train = y_train.to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be421bdc-aea6-48b0-8062-307bb1c0370a",
   "metadata": {},
   "source": [
    "We first start by fitting a logistic regression using the non-DP alogrithm from the scikit-learn library for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43f48607-89fa-4e3f-8589-deb173998da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9850746268656716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Create model\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Model's prediction performance (percentage of correctly classified labels):\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c9b41-507a-4179-8ef2-a04489e23567",
   "metadata": {},
   "source": [
    "We now attempt to fit a differentially private logistic regression using the DiffPrivLib library; [see doc](https://diffprivlib.readthedocs.io/en/latest/modules/models.html#logistic-regression):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6227af2-5e9a-4d3f-a0c3-105128ead2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffprivlib import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2882046e-d828-4481-bd3e-9cf269d7ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression model\n",
    "log_reg_model = models.LogisticRegression(epsilon = 1000.0, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63956dd1-1e00-418e-bfc3-fb48a159e649",
   "metadata": {},
   "source": [
    "We now fit the model on the training set and compute its accuracy; [doc here](https://diffprivlib.readthedocs.io/en/latest/modules/models.html#diffprivlib.models.LogisticRegression.score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d5010fd-f9bd-45e0-a617-e6831c01ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9402985074626866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/diffprivlib/models/logistic_regression.py:231: PrivacyLeakWarning: Data norm has not been specified and will be calculated on the data provided.  This will result in additional privacy leakage. To ensure differential privacy and no additional privacy leakage, specify `data_norm` at initialisation.\n",
      "  warnings.warn(\"Data norm has not been specified and will be calculated on the data provided.  This will \"\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "log_reg_model.fit(x_train, y_train)\n",
    "\n",
    "# Model's prediction performance (percentage of correctly classified labels):\n",
    "print(\"Accuracy:\", log_reg_model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00217a9a-b5bd-44c4-ba68-a3714aa54345",
   "metadata": {},
   "source": [
    "The model runs, is estimated and provide a performance comparable to the non-DP model, however the function return a worrisome privacy leak warning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9596f-c7d9-4b3f-b7e0-06a5cd0d8bbb",
   "metadata": {},
   "source": [
    "## Step 3': Logistic regression with explicit L2-norm bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aff436-8478-453f-8300-dcefd547e52b",
   "metadata": {},
   "source": [
    "We get a `PrivacyLeakWarning` warning because we did not specify the `data_norm` parameter. \n",
    "\n",
    "Differential privacy mechanisms need to know how much one individual’s record can change the model. In the case of the `LogisticRegression`, this depends on the maximum $L2$ norm of any row, which in turn depends on the size of feature vectors. `data_norm` is that bound: The maximum L2 norm of any single row (feature vector) in the dataset. \n",
    "\n",
    "If it is not specified, DiffPrivLib will infer if from the training data. As this compution is a query in itself, this may leak information about the dataset (e.g. what the max value was), hence the PrivacyLeakWarning. To avoid that, we can specify the bounds with `data_norm` using domain knowledge independent of the values in the dataset.\n",
    "\n",
    "We assume that the data originates from an official survey for which possible values of the field are considered public information, in particular:\n",
    "- bill length $\\in [30.0, 65.0]$,\n",
    "- bill depth $\\in [13.0, 23.0]$,\n",
    "- flipper length $\\in [150.0, 250.0]$,\n",
    "- body mass $\\in [2000.0, 7000.0]$,\n",
    "- sex $\\in [0, 1]$.\n",
    "\n",
    "Formally, for a row $x = (x_1, \\ldots, x_d)$, its L2 norm is $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_d^2}$.\n",
    "\n",
    "As the columns takes only positive values, the $L2$ norm of a row $x$ can be bounded using the feature maximum values:\n",
    "\n",
    "$$\n",
    "\\|x\\|_2 = \\sqrt{ (\\text{max bill length})^2 + (\\text{max bill depth})^2 + (\\text{max flipper length})^2 + (\\text{max body mass})^2 + (\\text{max sex})^2 }\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adccf7b-2361-4d15-9fa3-916244d4ede2",
   "metadata": {},
   "source": [
    "Writing all this information in a metadata dictionnary, we can then specify the `data_norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "932b16f3-b473-4d60-b296-ac3073b9e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88232fbf-0672-4930-8286-9cfc065c030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all metadata bounds\n",
    "metadata_bounds = {\n",
    "    'bill_length_mm': {'lower': 30.0, 'upper': 65.0},\n",
    "    'bill_depth_mm': {'lower': 13.0, 'upper': 23.0},\n",
    "    'sex': {'lower': 0.0, 'upper': 1.0}, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90437acb",
   "metadata": {},
   "source": [
    "The data norm is :\n",
    "\n",
    "$$\n",
    "\\|x\\|_2 = \\sqrt{ (\\text{max bill length})^2 + (\\text{max bill depth})^2 + (\\text{max sex})^2 }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1e1b6ee-933f-43e4-9085-7afd746ad9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68.95650803223725"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute the data_norm\n",
    "data_norm = (65**2 + 23**2+1**2)**(1/2)\n",
    "data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73334b38-d7b1-4c92-aee6-9877832f634c",
   "metadata": {},
   "source": [
    "We now re-define the logistic regression model using the pre-defined `data_norm` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9f60ee6-7621-4ad8-b88b-174dfb76a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_model_with_bounds = models.LogisticRegression(epsilon = 1e3, data_norm=data_norm, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0c6ed-9454-45aa-aeca-1e1e3cc175d7",
   "metadata": {},
   "source": [
    "We now fit the model on the training set and compute its accuracy; [doc here](https://diffprivlib.readthedocs.io/en/latest/modules/models.html#diffprivlib.models.LogisticRegression.score):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d30d2e9-2a6e-48f3-b9c4-bf47b197ba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9701492537313433\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "log_reg_model_with_bounds.fit(x_train, y_train)\n",
    "\n",
    "# Model's prediction performance (percentage of correctly classified labels):\n",
    "print(\"Accuracy:\", log_reg_model_with_bounds.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365003c-dba3-4d36-ac56-6226b05ba93f",
   "metadata": {},
   "source": [
    "You now obtained a model with a comparable prediction performance without the privacy leak warning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f630786a-d49b-4979-9b5c-294d000528d8",
   "metadata": {},
   "source": [
    "## Step 3'': Logistic regression with standard scaler pre-processing\n",
    "\n",
    "In the previous model, we trained logistic regression on raw features. Sometime re-scaling helps improving the model's performance.  \n",
    "Since the features have very different values' ranges (for example, body mass in the thousands versus sex as a binary 0/1), this can cause several issues:\n",
    "\n",
    "- the optimizer may converge more slowly  \n",
    "- model coefficients can become unbalanced  \n",
    "- in the case of differential privacy, very large row norms can occur, which increases the amount of noise added  \n",
    "\n",
    "To address this, we will create a DP-pipeline that first scales the data using a \n",
    "[`StandardScaler`](https://diffprivlib.readthedocs.io/en/latest/modules/models.html#standard-scaler), and then fits a logistic regression.  \n",
    "\n",
    "Scaling the features ensures they are on a comparable range, which improves optimization and reduces row norms.  \n",
    "This leads to better performance and more accurate differentially private estimates.  \n",
    "\n",
    "#### Step A - `StandardScaler`\n",
    "\n",
    "The `StandardScaler` model standardizes each feature by removing the mean and scaling to unit variance, but unlike scikit-learn’s version, the standardization is done under DP guarantees.\n",
    "\n",
    "The transformation applied on the raw data is: \n",
    "$$\n",
    "z = \\frac{x - u}{s}.\n",
    "$$\n",
    "where:\n",
    "- $u$ is the differentially private mean\n",
    "- $s$ is the differentially private standard deviation\n",
    "\n",
    "So the scaler will compute the mean and standard deviation for each re-scaled column using queries protected with a Laplace mechanism which requires proper calibration based on the column's metadata, but unlike the logistic regression model, the `StandardScaler` does not take a `data_norm` parameter. Instead, it requires `bounds` for each feature in order to determine the sensitivity and calibrate the amount of noise to add.  \n",
    "\n",
    "For a given column, the sensitivity is defined as $sens = \\text{max} - \\text{min}$.\n",
    "\n",
    "For example, consider the feature `bill_length_mm` with bounds in $[30.0, 65.0]$, the sensitivity is $sens = 65.0 - 30.0 = 35.0$.\n",
    "\n",
    "If we use the Laplace mechanism, the privatized query for this scaling this feature would be  \n",
    "\n",
    "$$\n",
    "u = data\\_average + \\text{Lap}\\left(0, \\frac{35.0}{\\epsilon}\\right), \\qquad s = data\\_std + \\text{Lap}\\left(0, \\frac{35.0}{\\epsilon}\\right)\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is the privacy budget.\n",
    "\n",
    "This ensures the mean and standard deviation are privatized, while the transformed data itself is not differentially private."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f477481-e160-4b38-8d6e-237cf74f5506",
   "metadata": {},
   "source": [
    "First, we compute the input feature data bounds of the standard scaler. Use the `get_bounds` function to instantiate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4292d03b-e79e-4bcd-93b5-be7efc2fdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning the columns bounds from the metadata\n",
    "def get_bounds(metadata_bounds, columns):\n",
    "    lower = [metadata_bounds[col][\"lower\"] for col in columns]\n",
    "    upper = [metadata_bounds[col][\"upper\"] for col in columns]\n",
    "    return (lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3eab1952-9eb5-47b3-8ca7-a554916255d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([30.0, 13.0, 0.0], [65.0, 23.0, 1.0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute the feature_bounds of the input data\n",
    "# The expected format is a tuple (list_of_lower_bounds, list_of_upper_bounds)\n",
    "feature_bounds = get_bounds(metadata_bounds, feature_columns)\n",
    "feature_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1778e0d-1e06-4d49-8f48-99ba04f1b840",
   "metadata": {},
   "source": [
    "We then instantiate and train the `StandardScaler` on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a7037fa-b198-4479-a7aa-f2f5eb8f36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = models.StandardScaler(epsilon = 0.5, bounds= feature_bounds)\n",
    "scaler = scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52bf65e-c29b-4ed7-b507-75e451fabb7e",
   "metadata": {},
   "source": [
    "And transform the training and testing data with the trained scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f692eed-bc94-4169-a14d-57fcc704c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_data = scaler.transform(x_train)\n",
    "scaled_test_data = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bacdda4-3d86-4744-b6aa-31d8aa8831b8",
   "metadata": {},
   "source": [
    "#### Step B - `LogisticRegression`\n",
    "After scaling, we apply the same differentially private logistic regression classifier.\n",
    "\n",
    "As before, this step requires a `data_norm`. \n",
    "\n",
    "However,\n",
    "- The StandardScaler transformed each feature as $z = \\frac{x - u}{s}$.\n",
    "- This changes the feature ranges, so the row norms (and their maximum possible value) also change.\n",
    "- To guarantee privacy correctly, we must compute the worst-case row norm after this transformation, based on:\n",
    "    - the original feature bounds,\n",
    "    - and the privatized mean and variance learned by the scaler.\n",
    "\n",
    "The new scaled bounds for each feature are obtained by applying the transformation to the original bounds. The maximum possible $L2$ norm across these bounds becomes the new `data_norm` to use in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72774801-7293-41f9-a9d3-6db35294e02a",
   "metadata": {},
   "source": [
    "We retrieve the trained model parameters (mean $u$ and variance $s^2$). They are available in the trained `StandardScaler` object with one value per feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5af72728-a1e1-4575-9fbc-ade55b320179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of bill_length_mm is 43.88187100593167.\n",
      "Mean of bill_depth_mm is 16.758204729112656.\n",
      "Mean of sex is 0.5101313841611681.\n"
     ]
    }
   ],
   "source": [
    "means = scaler.mean_\n",
    "for col, u in zip(feature_columns, means):\n",
    "    print(f\"Mean of {col} is {u}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "128d6574-ead4-4cd8-b4ee-8a755b42aa07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50.73095491, 15.44600251,  0.23429665])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Look at the StandardScaler documentation\n",
    "variances = scaler.var_\n",
    "variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4608a8aa-a329-4bf5-a2a2-777230dee0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of bill_length_mm is 50.7309549069747.\n",
      "Variance of bill_depth_mm is 15.446002505920678.\n",
      "Variance of sex is 0.23429665411580813.\n"
     ]
    }
   ],
   "source": [
    "for col, s in zip(feature_columns, variances):\n",
    "    print(f\"Variance of {col} is {s}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7366dbf-b341-47a2-94db-250688dc0e4d",
   "metadata": {},
   "source": [
    "And compute the new scaled bounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99e477cd-66c5-477e-a1b9-1d8bce7c5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_bounds(feature_bounds, means, variances):\n",
    "    \"\"\"\n",
    "    Compute new feature bounds after applying a StandardScaler transform:\n",
    "        z = (x - u) / s\n",
    "\n",
    "    Args:\n",
    "        feature_bounds: tuple (lower_bounds, upper_bounds), each a list of floats\n",
    "        means: list of feature means (u)\n",
    "        variances: list of feature variances\n",
    "    Returns:\n",
    "        scaled_bounds: tuple (scaled_lower_bounds, scaled_upper_bounds), each a list of floats\n",
    "    \"\"\"\n",
    "    lower, upper = feature_bounds\n",
    "    stds = np.sqrt(variances)\n",
    "\n",
    "    scaled_lower = []\n",
    "    scaled_upper = []\n",
    "\n",
    "    for l, u, mean, std in zip(lower, upper, means, stds):\n",
    "        z_lower = (l - mean) / std\n",
    "        z_upper = (u - mean) / std\n",
    "        scaled_lower.append(min(z_lower, z_upper))\n",
    "        scaled_upper.append(max(z_lower, z_upper))\n",
    "\n",
    "    return (scaled_lower, scaled_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27e9d56c-0aa2-424f-bfc1-3e3c7c7b8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_bounds = get_bounds(metadata_bounds, columns=feature_columns)\n",
    "scaled_bounds = get_scaled_bounds(feature_bounds, means, variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edcccd-6a22-4d6c-8690-b53f9a4b9263",
   "metadata": {},
   "source": [
    "Compute the new norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "740c05f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.float64(-1.9489984150869313),\n",
       "  np.float64(-0.9562520610712973),\n",
       "  np.float64(-1.053899017301703)],\n",
       " [np.float64(2.9649605533327152),\n",
       "  np.float64(1.588186387594794),\n",
       "  np.float64(1.0120374257867308)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83f1c1f6-6af2-41de-8364-97a1498d6a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.5124844249837572)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: compute the scaled_data_norm\n",
    "after_scaler_data_norm = np.sqrt(sum(s**2 for s in scaled_bounds[1]))\n",
    "after_scaler_data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a75173-6eea-4d2d-b25a-a80d77d52d3d",
   "metadata": {},
   "source": [
    "Now, we initialise the new classification model and train it on the scaled train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "924a3201-0feb-4ed8-b1a3-5977dc468a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9701492537313433\n"
     ]
    }
   ],
   "source": [
    "# Create LR model\n",
    "rescaled_log_reg_model =  models.LogisticRegression(epsilon = 100.0, data_norm = after_scaler_data_norm)\n",
    "\n",
    "# Fit the LR model\n",
    "rescaled_log_reg_model.fit(scaled_train_data, y_train)\n",
    "\n",
    "# Model's prediction performance (percentage of correctly classified labels):\n",
    "print(\"Accuracy:\", rescaled_log_reg_model.score(scaled_test_data, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0b1ab-fe22-4058-b79f-a463aff8da03",
   "metadata": {},
   "source": [
    "We obserbe that we get a similar performance than the model without rescaling, however we used a much lower privacy-loss budget to obtain a similar performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a37022-a6c6-493b-8165-18b7d7b9b076",
   "metadata": {},
   "source": [
    "We can look at the intercepts and estimated coefficients for the linear regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7f3df640-038b-4e20-924a-50896269d683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3, one for each species.\n",
      "Intercepts: [-0.72658593 -2.81396212 -0.93060715]\n"
     ]
    }
   ],
   "source": [
    "intercepts = rescaled_log_reg_model.intercept_\n",
    "print(f\"There are {len(intercepts)}, one for each species.\")\n",
    "print(f\"Intercepts: {intercepts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67f39521-ce66-4f7e-bc1b-385a6b199167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table of coefficient has of shape (3, 3), which corresponds to our 3 target species and 3 features.\n",
      "Coefficients: [[-4.75042398  2.00108181 -0.81760534]\n",
      " [ 3.22839751  3.79193632  1.73456577]\n",
      " [ 1.57241641 -5.79365071 -0.7142261 ]]\n"
     ]
    }
   ],
   "source": [
    "coefficients = rescaled_log_reg_model.coef_\n",
    "print(f\"The table of coefficient has of shape {coefficients.shape}, which corresponds to our 3 target species and {len(feature_columns)} features.\")\n",
    "print(f\"Coefficients: {coefficients}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674e345-9e6f-468d-9c38-99dc1661dc77",
   "metadata": {},
   "source": [
    "#### Step C - `Pipeline`\n",
    "Instead of training the standard scaler and logistic regression separatelly, it is possible to write a unique `pipeline` which combines the two models in an object.\n",
    "Write a pipeline with the two models below. \n",
    "\n",
    "Note: For this example, we can assume that the `after_scaler_data_norm` computed in step 2 is still valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "737c924a-82d6-4a82-a3c0-08ed7cd7ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75b10566-ee48-421b-9321-d2ffe215d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete pipeline and train it on the training data. \n",
    "dpl_pipeline_with_scaler = Pipeline([\n",
    "   ('scaler', models.StandardScaler(epsilon = 0.5, bounds= feature_bounds)),\n",
    "   ('classifier', models.LogisticRegression(epsilon = 100.0, data_norm = after_scaler_data_norm))\n",
    "])\n",
    "pipeline_with_scaler = dpl_pipeline_with_scaler.fit(scaled_train_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0ff31-4cea-4b37-bcd4-5a7dff621d74",
   "metadata": {},
   "source": [
    "And evaluate the score of this new model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "891ecf42-102a-4ca7-9ac7-b015f95a070e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.417910447761194"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Evaluate the score of this new pipeline\n",
    "sc_score = dpl_pipeline_with_scaler.score(scaled_test_data, y_test)\n",
    "sc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f631ba-e8c1-40ae-a5d4-07e79f828dc2",
   "metadata": {},
   "source": [
    "### Privacy loss budget accountant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031fbc73-75e3-4a05-bef8-10b9726c3408",
   "metadata": {},
   "source": [
    "Another important parameter in DiffPrivLib is the `accountant`. It enables to track the spent budget accross multiple steps in a pipeline and/or accross multiple pipelines.\n",
    "\n",
    "To do so, the same `accountant` instance of `BudgetAccountant` must be provided as input argument of the pipeline and/or models. \n",
    "\n",
    "We rewrite here the previous pipeline with an `accountant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5c4a459-ceb4-4f71-b603-47c48215c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffprivlib import BudgetAccountant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0ea5e75-012c-411b-88d1-3e93c6523011",
   "metadata": {},
   "outputs": [],
   "source": [
    "accountant = BudgetAccountant()\n",
    "\n",
    "pipeline_with_scaler = Pipeline([\n",
    "    ('scaler', models.StandardScaler(epsilon = 0.5, bounds=feature_bounds, accountant=accountant)),\n",
    "    ('classifier', models.LogisticRegression(epsilon = 1e2, data_norm=after_scaler_data_norm, accountant=accountant))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71460fd-af66-4d03-9893-802a5351cc48",
   "metadata": {},
   "source": [
    "With the `.total()` method, we can see the budget spent by the models. For now it was not fitted on the data so the budget is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e648745-260d-41ad-ba73-b9838571864d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(epsilon=0, delta=0.0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accountant.total()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b21ca9-0a5c-4f95-b955-68f31fc78173",
   "metadata": {},
   "source": [
    "We train the model on the data and check the budget again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58af5c36-2d28-49ea-b98c-bcd35bfffd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(epsilon=100.5, delta=0.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_scaler = pipeline_with_scaler.fit(x_train, y_train)\n",
    "accountant.total()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add13857-1a98-4c75-a99e-8f262d6137c4",
   "metadata": {},
   "source": [
    "As expected the budget from both steps is spent 0.5 + 1e2 = 100.5 epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f7e45-d07c-4077-9c96-4a3b453c7fe3",
   "metadata": {},
   "source": [
    "### Use the model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d735f16-d342-4dbf-8570-c64afa12be3b",
   "metadata": {},
   "source": [
    "We can use the model to predict a species depending on the feature values. \n",
    "\n",
    "For illustration, we make prediction for three hypothetical on female penguins with:\n",
    "- the smallest possible value for all features (lower bound)\n",
    "- the biggest possible value for all features (upper bound)\n",
    "- the medium value for all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7ce4fa2-1026-415a-b0c2-f19b500a08fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_length_mm  bill_depth_mm  sex\n",
       "0            30.0           13.0  0.0\n",
       "1            65.0           23.0  1.0\n",
       "2            47.5           18.0  0.5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature data\n",
    "x_to_predict_dict = {}\n",
    "for col in feature_columns:\n",
    "    lower = metadata_bounds[col]['lower']\n",
    "    upper = metadata_bounds[col]['upper']\n",
    "    midpoint = (lower + upper) / 2\n",
    "    x_to_predict_dict[col] = [lower, upper, midpoint]\n",
    "\n",
    "x_to_predict = pd.DataFrame(x_to_predict_dict)\n",
    "x_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcf56f97-8798-44ca-b3b4-4b46943b59c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>sex</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gentoo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Chinstrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Chinstrap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_length_mm  bill_depth_mm  sex predictions\n",
       "0            30.0           13.0  0.0      Gentoo\n",
       "1            65.0           23.0  1.0   Chinstrap\n",
       "2            47.5           18.0  0.5   Chinstrap"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute prediction with estimated model\n",
    "predictions = pipeline_with_scaler.predict(x_to_predict)\n",
    "x_to_predict[\"predictions\"] = predictions\n",
    "x_to_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711130a-893f-4f57-bbc2-4ac14ac9ba6b",
   "metadata": {},
   "source": [
    "Can you compute the prediction for the data in the test set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0246bb0-99be-41a2-a313-c3c7892592ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict the species of the data in the test set\n",
    "predictions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03779f-ecde-4498-94fb-3d7e188578a5",
   "metadata": {},
   "source": [
    "## Step 4: Clustering KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4753030-0a7c-43a7-9740-8cc65bf68131",
   "metadata": {},
   "source": [
    "DiffPrivLib also allows to use K-Means clustering [(see doc)](https://diffprivlib.readthedocs.io/en/latest/modules/models.html#k-means), i.e., that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean; see [link](https://en.wikipedia.org/wiki/K-means_clustering) for methodological details.\n",
    "\n",
    "We attempt to find `N_CLUSTERS` clusters using the features 'bill_length_mm', 'bill_depth_mm', and 'body_mass_g'. As it is unsupervised learning, there is no target column. For this reason, there is no need for a training and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1870046d-9563-47a7-8fc3-799dd0dc5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "N_CLUSTERS = 3\n",
    "feature_columns = ['bill_length_mm', 'bill_depth_mm', 'body_mass_g']\n",
    "feature_data = df[feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe52298-1b46-4c5a-8a2e-9f41c0759f7c",
   "metadata": {},
   "source": [
    "### StandardScaler and KMeans Pipeline\n",
    "Similarly to the logistic regression, we will first use a `StandardScaler` followed by `KMeans`, to improve the algorithm performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2726a31-c116-4b47-a16a-3ac7715b6274",
   "metadata": {},
   "source": [
    "As before, we need to determine all required parameter to avoid a `PrivacyLeakWarning`. For the `StandardScaler`, we get the input bounds from the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7acb612-4549-48f5-910f-f680dd18741d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([30.0, 13.0, 2000.0], [65.0, 23.0, 7000.0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all metadata bounds\n",
    "metadata_bounds = {\n",
    "    'bill_length_mm': {'lower': 30.0, 'upper': 65.0},\n",
    "    'bill_depth_mm': {'lower': 13.0, 'upper': 23.0},\n",
    "    'flipper_length_mm': {'lower': 150.0, 'upper': 250.0},\n",
    "    'body_mass_g': {'lower': 2000.0, 'upper': 7000.0},\n",
    "    'sex': {'lower': 0.0, 'upper': 1.0}, \n",
    "}\n",
    "\n",
    "# Exctract bounds\n",
    "feature_bounds = get_bounds(metadata_bounds, columns=feature_columns)\n",
    "feature_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bda52e-86c5-4a00-818a-af9384cf2c36",
   "metadata": {},
   "source": [
    "We instantiate and fit the `StandardScaler` and apply the standard scaling on the feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83cc26ba-2846-42f8-8067-28fc73e9f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler\n",
    "scaler = models.StandardScaler(epsilon = 10.0, bounds=feature_bounds)\n",
    "\n",
    "# Fit scaler\n",
    "scaler = scaler.fit(feature_data)\n",
    "\n",
    "# Rescale data\n",
    "scaled_feature_data = scaler.transform(feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6eedf-67bf-4362-82a0-66bc855da4d9",
   "metadata": {},
   "source": [
    "We also need to instantiate the `KMeans` pipeline. It will require a `bounds` parameter with the new scaled bounds (in output of the scaler) to be fed to the `K_Means` algorithm.\n",
    "\n",
    "Can you compute them ?\n",
    "Hint: You can use a similar logic as before and re-use the `get_scaled_bounds` function with the previously computed `means` and `variances`. Be careful, the bounds must be provided as a tuple of the form (list_lower_bounds, list_upper_bounds), like `feature_bounds` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "043d6a75-abe0-467c-bb9a-9c38a140ad00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([np.float64(-1.9489984150869313),\n",
       "  np.float64(-0.9562520610712973),\n",
       "  np.float64(4130.818987159566)],\n",
       " [np.float64(2.9649605533327152),\n",
       "  np.float64(1.588186387594794),\n",
       "  np.float64(14460.501202601736)])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Instantiate the bounds after the `StandardScaler`\n",
    "after_scaler_bounds = get_scaled_bounds(feature_bounds, means, variances)\n",
    "after_scaler_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78af5e58-b3d4-4b7c-8030-234a5a393037",
   "metadata": {},
   "source": [
    "We have now everything to instantiate the k-means pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63cda8f2-da82-43d6-b6f6-5021e5c31f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "accountant = BudgetAccountant()\n",
    "kmeans_pipeline = Pipeline([\n",
    "    ('scaler', models.StandardScaler(epsilon = 10.0, bounds=feature_bounds, accountant=accountant)),\n",
    "    ('kmeans', models.KMeans(n_clusters = N_CLUSTERS, epsilon = 50.0, bounds= after_scaler_bounds, accountant=accountant)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa1089b-e1cd-49c0-b76b-eb3fd9a05364",
   "metadata": {},
   "source": [
    "Let's fit the K-Meand model on the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "afe907e1-324a-45d2-9f66-1d1d3e879c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the K-Mean pipeline\n",
    "kmeans_pipeline = kmeans_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0894058-48a1-4e0c-a7ea-539152a3bdbf",
   "metadata": {},
   "source": [
    "We can look at the spent budget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4706c-fd12-443a-bd30-65cabd6e2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accountant.total()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9c0247-e485-41c5-a7ec-066beea8cd7a",
   "metadata": {},
   "source": [
    "And at the score of the model, which is the opposite of the value of X on the K-means objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fe0e9-c558-466a-9e58-9698a0321952",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pipeline.score(feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f02aa7-0f8d-4a24-9912-a933211daa81",
   "metadata": {},
   "source": [
    "FYI: According to the official [documentation](https://diffprivlib.readthedocs.io/en/latest/modules/models.html#diffprivlib.models.KMeans.score), the `score` of the `K-means` model is the 'opposite of the value of X on the K-means objective'.\n",
    "\n",
    "Ususally, the K-means objective is the *inertia*, i.e. the within-cluster sum of squared distances:\n",
    "\n",
    "$$\n",
    "\\text{Inertia} = \\sum_{i=1}^n \\min_{c \\in \\text{clusters}} \\lVert x_i - \\mu_c \\rVert^2\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $x_i$ = a data point  \n",
    "- $\\mu_c$ = the centroid of cluster $c$  \n",
    "\n",
    "The K-means algorithm tries to minimize inertia (tight, well-separated clusters), where a smaller value of `Inertia` is better.\n",
    "\n",
    "DiffPrivLib is based on scikit-learn and in scikit-learn, by convention, higher scores are always better. Hence, to stay consistent, scikit-learn defines: $\\text{score}(X) = - \\text{Inertia}(X)$, which represent the same information, just with opposite signs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23998e64-3047-4643-832b-1f2cbd04685b",
   "metadata": {},
   "source": [
    "We can use the model to predict the clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e26cd3-42e2-4179-85b1-8551d3efd571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict clusters on feature_data with K-Mean pipeline\n",
    "predictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a6d65-f50f-434f-a50e-3dd4758c8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"predictions\"] = predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e11c7-bc14-444d-a69d-5752d75cdc9e",
   "metadata": {},
   "source": [
    "We can plot the results to study the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf7c65-34b8-4d51-9c39-8e66e5487be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccebade9-0f9e-4ccf-9354-bbd475340427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df, x, y, cluster_col=\"predictions\", style_col=\"species\"):\n",
    "    \"\"\"\n",
    "    Plot two features colored by cluster predictions and shaped by species.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the features, cluster predictions, and species.\n",
    "    x : str\n",
    "        Feature name for the x-axis.\n",
    "    y : str\n",
    "        Feature name for the y-axis.\n",
    "    cluster_col : str, default=\"predictions\"\n",
    "        Column name with cluster assignments.\n",
    "    style_col : str, default=\"species\"\n",
    "        Column name for marker style (e.g., species).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=cluster_col,\n",
    "        style=style_col,\n",
    "        palette=\"deep\",\n",
    "        s=80\n",
    "    )\n",
    "    plt.title(f\"DP KMeans Clustering: {x} vs {y}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081e752a-8a67-444f-b599-0ed8338aabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(df, x=\"bill_length_mm\", y=\"flipper_length_mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31724de2-2cc7-4451-a9c9-8431959150dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(df, x=\"body_mass_g\", y=\"bill_length_mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ab96a-275b-4440-9fab-79b1930d72c9",
   "metadata": {},
   "source": [
    "Feel free to try various number of cluster options, feature selection and axis to plot!\n",
    "\n",
    "We can check how the DP K-means result compare to the non-DP alogrithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fe154-b41d-4992-ba36-03945390a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(feature_data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# 3. Predict cluster assignments\n",
    "y_kmeans = kmeans.predict(X_scaled)\n",
    "\n",
    "df[\"predictions_nonDP\"] = y_kmeans\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c198e5d-02d3-46e9-abb6-580e3cb5787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(df, x=\"bill_length_mm\", y=\"flipper_length_mm\", cluster_col=\"predictions_nonDP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0f3a3-4ab2-45b2-8329-69074ef76701",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(df, x=\"body_mass_g\", y=\"bill_length_mm\", cluster_col=\"predictions_nonDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27f78e-d6d0-4fdc-b28d-50740f2ffac9",
   "metadata": {},
   "source": [
    "We see that we get similar results with the DP and non-DP clustering! You can try to reduce the privacy loss budget to see the algorithm's performance decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61288218-4a6d-474c-a360-e9e4b39a8477",
   "metadata": {},
   "source": [
    "## KMeans after PCA and StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5b7f34-d006-4b2c-b361-b212aef05326",
   "metadata": {},
   "source": [
    "In cases where the number of columns in the data is large, alogrithms like k-means performs better if some dimension reduction is done beforehand.\n",
    "\n",
    "We now try to do a dimensionality reduction to two components with a PCA before K-Means to see if we can obtain similar results when using all initial features. If you are unfamiliar with `PCA`, [this page](https://builtin.com/data-science/step-step-explanation-principal-component-analysis) provides a good explanation.\n",
    "\n",
    "The PCA model requires `bounds` and `data_norm` parameters. Using the same logic as before, re-instantiate the appropriate parameters in a first pipeline with a `StandardScaler` followed by a `PCA` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2657ea4-ff9a-4886-9d93-d97967b5fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of target dimensions\n",
    "N_COMPONENTS = 5\n",
    "\n",
    "# Selecte all featurs as input\n",
    "feature_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']\n",
    "feature_data_pca = df[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c155d-7d51-4257-9a63-d0f2c9d772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all metadata bounds\n",
    "metadata_bounds = {\n",
    "    'bill_length_mm': {'lower': 30.0, 'upper': 65.0},\n",
    "    'bill_depth_mm': {'lower': 13.0, 'upper': 23.0},\n",
    "    'flipper_length_mm': {'lower': 150.0, 'upper': 250.0},\n",
    "    'body_mass_g': {'lower': 2000.0, 'upper': 7000.0},\n",
    "    'sex': {'lower': 0.0, 'upper': 1.0}, \n",
    "}\n",
    "\n",
    "# Exctract bounds\n",
    "feature_bounds = get_bounds(metadata_bounds, columns=feature_columns)\n",
    "feature_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4b682-ba28-43eb-af2c-1231747a3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler\n",
    "scaler = models.StandardScaler(epsilon = 10.0, bounds=feature_bounds)\n",
    "\n",
    "# Rescale data\n",
    "scaled_feature_data_pca = scaler.fit_transform(feature_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fc2db-8120-46e2-92b0-783db36040fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Using what we learned above, re-instantiate the appropriate bounds for the pipeline below\n",
    "after_scaler_bounds_pca = ...\n",
    "after_scaler_data_norm_pca = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd016aef-98fa-4b2d-8e3a-9271ef2f769e",
   "metadata": {},
   "source": [
    "We instantiate a budget accountant and define the first two steps of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f65b8c-6aa9-4ffd-9c3d-76d0ac09eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "accountant = BudgetAccountant()\n",
    "\n",
    "pca_pipeline = Pipeline([\n",
    "    ('scaler', models.StandardScaler(epsilon = 10.0, bounds=feature_bounds, accountant=accountant)),\n",
    "    ('pca', models.PCA(\n",
    "        n_components=N_COMPONENTS,\n",
    "        epsilon=1e3,\n",
    "        bounds=after_scaler_bounds_pca,\n",
    "        data_norm=after_scaler_data_norm_pca,\n",
    "        accountant=accountant)\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e032f-207e-4e69-89bc-40b2c0a4e9bb",
   "metadata": {},
   "source": [
    "For now we have not spent any budget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81e4ca-3111-4852-9cd2-7b607dbd7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "accountant.total()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36436715-7961-47d9-8880-ec582a381b34",
   "metadata": {},
   "source": [
    "We fit the pipeline on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c8321-2a82-4e4f-b5f7-fa667a712205",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipeline = pca_pipeline.fit(feature_data_pca)\n",
    "pca_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4a4466-1981-4f3c-a418-e4cce7d62d78",
   "metadata": {},
   "source": [
    "How much budget have we spent now ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c0781-c391-41e5-921d-bbd6a9944a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How much budget was spent for this training ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f096348-8b34-4350-bbed-8b6b17061613",
   "metadata": {},
   "source": [
    "In a `pipeline`, each `model` is in a `step`. We can retrieve the PCA model (second step) to look at the some of the trained parameters [(see doc)](https://diffprivlib.readthedocs.io/en/latest/modules/models.html#pca):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2156faa-c47b-441a-9410-2f109d4c02d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = pca_pipeline.steps[1][1]\n",
    "pca_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4b4fd-7974-4a1f-849b-1111daaa07bd",
   "metadata": {},
   "source": [
    "We can look at the amount of variance explained by each of the selected components. Or in other words, the eigenvalues $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e14a7-42b7-4ea3-8336-eb0e5b860da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419f2b5-05dc-4abc-8aaf-d71f8b8cf16a",
   "metadata": {},
   "source": [
    "And the percentage of variance explained by each of the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4459863-6f9c-4a81-92ba-c00ae440313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55638990-c6e0-4c4f-a3df-bd64ca5404ed",
   "metadata": {},
   "source": [
    "We thus will keep only the first 4 components as those contain most of the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba7c9d5-0b42-49ec-90a0-6f1ce3f35c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHOSEN_COMPONENTS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aacdd40-7cc6-462c-a723-615a1f374809",
   "metadata": {},
   "source": [
    "The per-feature empirical mean is now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122c2d9-0eaa-41b1-b631-8ccf99febaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mean = pca_model.mean_\n",
    "pca_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda38205-20bc-40bf-ae95-0b94b17034ec",
   "metadata": {},
   "source": [
    "And finally the principal axes in feature space, representing the directions of maximum variance in the data. Or in other words, the eigenvectors of the covariance matrix of shape `(n_components, n_features)`, where each row is a principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a648b7-9a56-4f0b-a755-c5d90bd3f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = pca_model.components_\n",
    "pca_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062e06b1-93c6-458b-9e54-5b22b9e7bebc",
   "metadata": {},
   "source": [
    "We can use the [transform](https://diffprivlib.readthedocs.io/en/latest/modules/models.html#diffprivlib.models.PCA.transform) method to apply the pipeline (`StandardScaler` + `PCA`) on the feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7779c-cb12-462b-85e1-7f0fe53f09ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_feature_data = pca_pipeline.transform(feature_data_pca)\n",
    "pca_feature_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa29ac0-2ac8-4da2-aa34-73e22c13ce6a",
   "metadata": {},
   "source": [
    "This outputs 2 columns that we add to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a2a25-04eb-4815-9f79-b89aaf854f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"component_1\", \"component_2\",  \"component_3\",  \"component_4\",  \"component_5\"]] = pd.DataFrame(pca_feature_data, index=df.index)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c4b2b5-debc-4d4d-9614-93b045e669b2",
   "metadata": {},
   "source": [
    "We now want to train a K-Means clustering on the principal components. \n",
    "\n",
    "Once again, we need to determine bounds for the data output of PCA and the input of K-Means. Luckily, we already have everything we need!\n",
    "\n",
    "**Note: This is more advanced and you can skip the explanation if you are only interested in the implementation.**\n",
    "\n",
    "In our pipeline, we first applied a `StandardScaler`. After this step, each feature has known bounds `after_scaler_bounds`.\n",
    "\n",
    "PCA then projects the scaled data onto a lower-dimensional space:\n",
    "\n",
    "$$\n",
    "Z = (X_\\text{scaled} - \\text{pca\\_mean}) \\cdot V^T\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $V$ = Matrix of eigenvectors (`pca_components` variable)\n",
    "- $\\text{pca\\_mean}$ = Per feature mean of the scaled data (`pca_model.mean_` variable)\n",
    "- $X_\\text{scaled}$ = Data output of standard scaler (input of PCA)\n",
    "- $Z$ = transformed data in lower dimensions  (`N_COMPONENTS` dimensions)\n",
    "\n",
    "Each PCA component is a linear combination of the scaled features:\n",
    "\n",
    "$$\n",
    "z_j = \\sum_i V_{j,i} \\, (x_i^\\text{scaled} - \\text{pca\\_mean}_i)\n",
    "$$\n",
    "\n",
    "where \n",
    "- $\\text{pca\\_mean}_i$ is the mean of the $i$-th feature used internally by PCA\n",
    "- $V_{i,j}$ is the weight of feature $i$ in principal component $j$.\n",
    "\n",
    "Each scaled feature $x_i^\\text{scaled}$ lies in a known interval $[l_i, u_i]$, so the transformed PCA component $z_j$ also lies in some interval $[z_j^\\text{min}, z_j^\\text{max}]$.  \n",
    "Since $z_j$ is a linear combination of the features, the extreme values of $z_j$ occur when each feature is at either its lower or upper bound.  \n",
    "\n",
    "By summing these extreme values across all features, we get the lower and upper bounds for $z_j$:\n",
    "\n",
    "$$\n",
    "z_j^\\text{min} = \\sum_i V_{j,i} \\cdot \n",
    "\\begin{cases} \n",
    "l_i - \\text{pca\\_mean}_i & \\text{if } V_{j,i} \\ge 0 \\\\ \n",
    "u_i - \\text{pca\\_mean}_i & \\text{if } V_{j,i} < 0 \n",
    "\\end{cases}, \n",
    "\\quad\n",
    "z_j^\\text{max} = \\sum_i V_{j,i} \\cdot \n",
    "\\begin{cases} \n",
    "u_i - \\text{pca\\_mean}_i & \\text{if } V_{j,i} \\ge 0 \\\\ \n",
    "l_i - \\text{pca\\_mean}_i & \\text{if } V_{j,i} < 0 \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567c4b6-122b-446b-b591-3552208524aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca_bounds(feature_bounds, pca_mean, pca_components, n_components):\n",
    "    \"\"\"\n",
    "    Compute PCA component bounds from feature bounds and PCA model.\n",
    "    \n",
    "    Args:\n",
    "        feature_bounds: tuple (lower_bounds, upper_bounds) after scaling\n",
    "        pca_mean: array-like of shape [n_features], mean of PCA model\n",
    "        pca_components: array-like of shape [n_components, n_features]\n",
    "        n_components: integer for the number of components to use\n",
    "    \n",
    "    Returns:\n",
    "        pca_bounds: tuple (lower_bounds, upper_bounds) per PCA component\n",
    "        max_row_norm: worst-case L2 norm across components\n",
    "    \"\"\"\n",
    "    lower, upper = feature_bounds\n",
    "    \n",
    "    pca_lower = []\n",
    "    pca_upper = []\n",
    "    \n",
    "    for j in range(n_components):\n",
    "        z_min = 0\n",
    "        z_max = 0\n",
    "        for i, V_ji in enumerate(pca_components[j]):\n",
    "            l_i = lower[i] - pca_mean[i]\n",
    "            u_i = upper[i] - pca_mean[i]\n",
    "            if V_ji >= 0:\n",
    "                z_min += V_ji * l_i\n",
    "                z_max += V_ji * u_i\n",
    "            else:\n",
    "                z_min += V_ji * u_i\n",
    "                z_max += V_ji * l_i\n",
    "        pca_lower.append(z_min)\n",
    "        pca_upper.append(z_max)\n",
    "    \n",
    "    return pca_lower, pca_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9b3f0-dbdf-4ba8-8539-59eb47a36907",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_pca_bounds = get_pca_bounds(after_scaler_bounds_pca, pca_mean, pca_components, N_CHOSEN_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83bc19-85f0-4ac7-a69f-42e159f58f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate a KMeans model with N_CLUSTERS, epsilon as 2.0, the right bounds (it is after the PCA) and the same privacy budget accountant as for the `pca_pipeline`\n",
    "kmeans_model = models.KMeans(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d393e-8528-4ab7-91af-149b2b0962d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit the KMeans model on the principal components\n",
    "kmeans_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f6524-6118-421d-ad27-aebaae186d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Predict clusters on principal compoents with K-Mean pipeline\n",
    "after_pca_predictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161203d4-8cfe-4ffd-854f-496f5fa497f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"after_pca_predictions\"] = after_pca_predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e7840-e4b2-4bd7-b417-81fe60db6bc1",
   "metadata": {},
   "source": [
    "We can now plot the results along the principal components axix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7ff8e-c6f1-4dbd-9003-1f1bdc076afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(df, x=\"component_1\", y=\"component_2\", cluster_col=\"after_pca_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b58f4-7eb5-4523-a20a-a91f29d06fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(df, x=\"component_3\", y=\"component_4\", cluster_col=\"after_pca_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53cba96-55c3-42b0-82c4-1ee518224f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(df, x=\"component_1\", y=\"component_3\", cluster_col=\"after_pca_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eaf0da-8a7a-4d46-8762-9c68c1d69e65",
   "metadata": {},
   "source": [
    "Again, feel free to try various number of principal components, cluster options, feature selection and axis to plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12695d16-ef59-4e7a-90af-54524c392ac4",
   "metadata": {},
   "source": [
    "Let's now check what was the total budget spent by the scaler, pca and k-means models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81ceb0-eb89-4fd2-a27c-6e90df3ce308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get epsilon and delta (hint: use the accountant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d2996-10d0-492f-aae0-13f2a53ab2a9",
   "metadata": {},
   "source": [
    "## Step 5: On your Own\n",
    "\n",
    "DiffPrivLib also provides additional models:\n",
    "- Gaussian Naive Bayes, \n",
    "- Random Forest Classifier,\n",
    "- Decision Tree Classifier and\n",
    "- Linear Regressions.\n",
    "\n",
    "Using these models is very similar to the examples shown here. Apart from the parameters already introduced (`epsilon`, `bounds` and `data_norm`), no additional differentially private settings are required.\n",
    "\n",
    "Feel free to experiment with these models on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d9c6f-cead-470f-a65e-43b665188345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
